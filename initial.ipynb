{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1264c221ec993ccd",
   "metadata": {},
   "source": [
    "# Our First Step: Run the Original Model\n",
    "\n",
    "We are using `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` mainly in our experiment, which is a reasoning model based on Qwen-2.5B.\n",
    "We are planning to evaluate Coconut to the distilled model, and we will compare the performance of Coconut with the original CoT.\n",
    "\n",
    "## Import Dependencies and the Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "f645d522ed6006cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T15:53:43.139941Z",
     "start_time": "2025-03-13T15:53:26.496741Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", device_map=device, torch_dtype=torch.float16)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Datasets",
   "id": "a599084aa2449315"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T15:53:13.802328Z",
     "start_time": "2025-03-13T15:53:08.585799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"open-r1/OpenThoughts-114k-math\")\n",
    "\n",
    "ds['train']"
   ],
   "id": "488ad16f8a77dd6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'problem', 'solution', 'messages', 'system', 'conversations', 'generated_token_count', 'correct'],\n",
       "    num_rows: 89120\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modify the CoT Chain to use `<sot>` and `<eot>`\n",
    "\n",
    "In DeepSeek R1, the CoT chain is wrapped with `<think>` and `</think>` XML tag, but we will use `<sot>` and `<eot>` special tokens instead.\n",
    "\n",
    "That's because in Coconut, we must find a **special** thing to determine whether the chain is terminated or notâ€”in legacy CoT, we can just use HTML parsing and use `</think>` to determine the end of the chain. However, this doesn't apply to Coconut.\n",
    "\n",
    "Hence, we will use `<sot>` and `<eot>` to determine the start and end of the chain.\n",
    "\n",
    "## Add Special Tokens"
   ],
   "id": "707628c234e1d535"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T01:11:35.866495Z",
     "start_time": "2025-03-14T01:11:35.554067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SOT: Start of Thought, EOT: End of Thought, SOS: Start of Solution, EOS: End of Solution\n",
    "special_tokens_dict = {'additional_special_tokens': ['<sot>', '<eot>', '<sos>', '<eos>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "id": "8198fd89fd3fea4a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(151667, 1536)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocess the Dataset\n",
    "\n",
    "We will preprocess the dataset to add `<sot>` and `<eot>` to the CoT chain."
   ],
   "id": "6291593a597d4369"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T01:32:24.082223Z",
     "start_time": "2025-03-14T01:29:33.981469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "ft_ds = load_dataset('ServiceNow-AI/R1-Distill-SFT', 'v1')\n",
    "\n",
    "def replace_tags(example):\n",
    "    example['reannotated_assistant_content'] = re.sub(r'<think>(.*?)</think>', r'<sot>\\1<eot>', example['reannotated_assistant_content'])\n",
    "    example['reannotated_assistant_content'] = re.sub(r'\\\\boxed{(.*?)}', r'<sos>\\1<eos>', example['reannotated_assistant_content'])\n",
    "    return example\n",
    "\n",
    "\n",
    "ft_ds = ft_ds.map(replace_tags)"
   ],
   "id": "ede82c9eaeb46209",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df856c0343194bb2bda5e203c0cba01e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/51 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7e1d9e9c6594d54b9fa8cc86e11834f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1679162 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1d78e8886f74232a220da8b8a481a9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Due to the set size, we can use the 1024 items of the set to test the model.",
   "id": "a6d341da22da2f39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T01:38:04.347151Z",
     "start_time": "2025-03-14T01:37:52.667503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_subset = ft_ds['train'].shuffle(seed=42).select(range(1024))\n",
    "\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"reannotated_assistant_content\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=32768\n",
    "    )\n",
    "    tokens = {k: torch.tensor(v).to(device) for k, v in tokens.items()}  # Move to MPS\n",
    "    return tokens\n",
    "\n",
    "tokenized_ds = train_subset.map(tokenize_function, batched=True)\n",
    "tokenized_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ],
   "id": "d2caa3eb1a43e0db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74bf2d1fcc404fc0a7c2f09e829b3965"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Try to train the model",
   "id": "4ac982cc1fec841f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T01:40:12.632808Z",
     "start_time": "2025-03-14T01:40:08.465520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./stage_1\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,  # 3 epochs to make use of 1000 examples\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "class MPS_Trainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to MPS\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = MPS_Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "6094dfad04d7df5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/btxkh02s2tz8drtw_bzqrzsc0000gn/T/ipykernel_99053/725206940.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MPS_Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = MPS_Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 16.22 GB, other allocations: 816.00 KB, max allowed: 18.13 GB). Tried to allocate 2.00 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 34\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (loss, outputs) \u001B[38;5;28;01mif\u001B[39;00m return_outputs \u001B[38;5;28;01melse\u001B[39;00m loss\n\u001B[1;32m     27\u001B[0m trainer \u001B[38;5;241m=\u001B[39m MPS_Trainer(\n\u001B[1;32m     28\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     29\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m     30\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtokenized_ds,\n\u001B[1;32m     31\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer\n\u001B[1;32m     32\u001B[0m )\n\u001B[0;32m---> 34\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/transformers/trainer.py:2241\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2239\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[1;32m   2242\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   2243\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[1;32m   2244\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m   2245\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[1;32m   2246\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/transformers/trainer.py:2548\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2541\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2542\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2543\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2544\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2545\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2546\u001B[0m )\n\u001B[1;32m   2547\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2548\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs, num_items_in_batch)\n\u001B[1;32m   2550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2551\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2552\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2553\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2554\u001B[0m ):\n\u001B[1;32m   2555\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2556\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/transformers/trainer.py:3698\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3695\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3697\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3698\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(model, inputs, num_items_in_batch\u001B[38;5;241m=\u001B[39mnum_items_in_batch)\n\u001B[1;32m   3700\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3701\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   3702\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3703\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   3704\u001B[0m ):\n",
      "Cell \u001B[0;32mIn[27], line 23\u001B[0m, in \u001B[0;36mMPS_Trainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_loss\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, inputs, return_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, num_items_in_batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     22\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mitems()}  \u001B[38;5;66;03m# Move inputs to MPS\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[1;32m     24\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (loss, outputs) \u001B[38;5;28;01mif\u001B[39;00m return_outputs \u001B[38;5;28;01melse\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:856\u001B[0m, in \u001B[0;36mQwen2ForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    853\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    855\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 856\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\n\u001B[1;32m    857\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    858\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m    859\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m    860\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m    861\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m    862\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m    863\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    864\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m    865\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m    866\u001B[0m     cache_position\u001B[38;5;241m=\u001B[39mcache_position,\n\u001B[1;32m    867\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    868\u001B[0m )\n\u001B[1;32m    870\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    871\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:549\u001B[0m, in \u001B[0;36mQwen2Model.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m position_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    547\u001B[0m     position_ids \u001B[38;5;241m=\u001B[39m cache_position\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m--> 549\u001B[0m causal_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_causal_mask(\n\u001B[1;32m    550\u001B[0m     attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n\u001B[1;32m    551\u001B[0m )\n\u001B[1;32m    553\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m inputs_embeds\n\u001B[1;32m    555\u001B[0m \u001B[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:668\u001B[0m, in \u001B[0;36mQwen2Model._update_causal_mask\u001B[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001B[0m\n\u001B[1;32m    661\u001B[0m     target_length \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    662\u001B[0m         attention_mask\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    663\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(attention_mask, torch\u001B[38;5;241m.\u001B[39mTensor)\n\u001B[1;32m    664\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m past_seen_tokens \u001B[38;5;241m+\u001B[39m sequence_length \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    665\u001B[0m     )\n\u001B[1;32m    667\u001B[0m \u001B[38;5;66;03m# In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\u001B[39;00m\n\u001B[0;32m--> 668\u001B[0m causal_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_4d_causal_attention_mask_with_cache_position(\n\u001B[1;32m    669\u001B[0m     attention_mask,\n\u001B[1;32m    670\u001B[0m     sequence_length\u001B[38;5;241m=\u001B[39msequence_length,\n\u001B[1;32m    671\u001B[0m     target_length\u001B[38;5;241m=\u001B[39mtarget_length,\n\u001B[1;32m    672\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[1;32m    673\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m    674\u001B[0m     cache_position\u001B[38;5;241m=\u001B[39mcache_position,\n\u001B[1;32m    675\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39minput_tensor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    676\u001B[0m     config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig,\n\u001B[1;32m    677\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m    678\u001B[0m )\n\u001B[1;32m    680\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    681\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msdpa\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    682\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    687\u001B[0m     \u001B[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001B[39;00m\n\u001B[1;32m    688\u001B[0m     \u001B[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001B[39;00m\n\u001B[1;32m    689\u001B[0m     causal_mask \u001B[38;5;241m=\u001B[39m AttentionMaskConverter\u001B[38;5;241m.\u001B[39m_unmask_unattended(causal_mask, min_dtype)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/coconut/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:749\u001B[0m, in \u001B[0;36mQwen2Model._prepare_4d_causal_attention_mask_with_cache_position\u001B[0;34m(attention_mask, sequence_length, target_length, dtype, device, cache_position, batch_size, config, past_key_values)\u001B[0m\n\u001B[1;32m    747\u001B[0m causal_mask \u001B[38;5;241m=\u001B[39m causal_mask[\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, :, :]\u001B[38;5;241m.\u001B[39mexpand(batch_size, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    748\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 749\u001B[0m     causal_mask \u001B[38;5;241m=\u001B[39m causal_mask\u001B[38;5;241m.\u001B[39mclone()  \u001B[38;5;66;03m# copy to contiguous memory for in-place edit\u001B[39;00m\n\u001B[1;32m    750\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attention_mask\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m target_length:\n\u001B[1;32m    751\u001B[0m         attention_mask \u001B[38;5;241m=\u001B[39m attention_mask[:, :target_length]\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 16.22 GB, other allocations: 816.00 KB, max allowed: 18.13 GB). Tried to allocate 2.00 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
